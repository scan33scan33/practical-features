{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A practical guide to feature Engineering\n",
    "This document dicusses some common ways of doing feature engineerings in machine learning with working code whenever possible. The scikit-learn package is used throughout the document. Many common skils for feature engineering are already in the scikit-learn page (http://scikit-learn.org/stable/modules/preprocessing.html.) The goal of this document is to talk about the skills I use, and my perspectives about them.\n",
    "\n",
    "## Feature Engineering\n",
    "The goal of feature engineering here includes making features more intepretable, making classification accuracy higher, etc.\n",
    "\n",
    "## Setup\n",
    "In the following experiments, we will fix the dataset (iris) and the classifier (Liblinear) to play with. For evaluation, 5-fold cross-validation is used for accuracy computation. Although our toy dataset may seem small, the code provided should work for larger data with reasonable speed. (For example, I have run the scripts on dense data with 20k samples and 20-dim features, and the scripts can finish in one second.) Let's load the data and classiifer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 150\n",
      "Number of feature: 4\n",
      "Labels: set([0, 1, 2])\n",
      "Cross-validation accuracies: 0.953333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import load_iris\n",
    "classifier = svm.LinearSVC(penalty='l1', dual=False)\n",
    "data = load_iris()  # TODO for readers: feel free to replace this line with any dataset that you have in mind.\n",
    "print \"Number of samples:\", len(data.data)\n",
    "print \"Number of feature:\", len(data.data[0])\n",
    "print \"Labels:\", set(data.target)\n",
    "labels = data.target\n",
    "features = data.data\n",
    "\n",
    "def DoCrossValidation(classifier, features, labels):\n",
    "    from sklearn.cross_validation import cross_val_score\n",
    "    return sum(cross_val_score(classifier, features, labels, cv=5)) / 5\n",
    "print \"Cross-validation accuracies:\", DoCrossValidation(classifier, features, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques\n",
    "### Categorical Feature Processing\n",
    "As many classifiers(, ranging from linear classifiers to neural networks,) assign a weight to a feature at some stage of their classifier training, it is important that the weight should make sense. \n",
    "#### Categorical Feature Expansion\n",
    "A common way to make categorical features make sense is to have each category to take an indivial dimension. For example, if we want to predict if a person has diabete using which state he/she is from as a feature. It make more sense to have 50 features where each feature represent a state than having a single feature that includes all state. That way, the resulting will be something like [0.8 * (indicator if this person is in CA), 0.2 * (indicator if this person is in NY), ...] rather than 0.5 * (a number representing the state). (For more information about such classification tasks see: https://github.com/scan33scan33/CoUS.) \n",
    "When it comes to coding, DictVectorizer is a great tool for categorical feature expansion. Let's assume the dimension zero is categorical. Here is how I will do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "def MakeFeatureDict(data):\n",
    "    dicted_features = []\n",
    "    for sample in data:\n",
    "        dicted_feature = {}\n",
    "        for kv in enumerate(sample):\n",
    "            dicted_feature[kv[0]] = kv[1]\n",
    "        dicted_features.append(dicted_feature)\n",
    "    return dicted_features\n",
    "\n",
    "# Marks a dimension 'dim' in a list of feature dictionaries as str to be considered categorical for DictVectorizer.\n",
    "def MarkDimAsCategorical(dicted_features, dim):\n",
    "    dicted_features_ret = []\n",
    "    for sample in dicted_features:\n",
    "        new_sample = sample.copy()\n",
    "        if dim in sample:\n",
    "            new_sample[dim] = str(sample[dim])\n",
    "        dicted_features_ret.append(new_sample)\n",
    "    return dicted_features_ret\n",
    "\n",
    "dicted_features = MakeFeatureDict(features)\n",
    "dicted_features = MarkDimAsCategorical(dicted_features, 0)  # TODO for readers: change the feature dimension here.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "temp_features = vectorizer.fit_transform(dicted_features)\n",
    "print \"Cross-validation accuracies:\", DoCrossValidation(classifier, temp_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Scaling is important for two purposes, balancing out the effect of different features and making weights on individual features make sense.\n",
    "#### First Purpose: Balancing out the Effect of Different Features\n",
    "I usually don't care much about the first purpose because I belive it is something the models should learn. However, we have to remember that different features may be of different scales when we interpret weights. For example, a weight 0.2 on a feature ranged in [0, 100] may be more important than a weight 1 on a feature ranged in [0, 1]. Just for my convenience, I usually just scale all features to [0, 1]. Here are some examples on how I will do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "temp_features = min_max_scaler.fit_transform(features)\n",
    "print \"Cross-validation accuracies:\", DoCrossValidation(classifier, temp_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Purpose: Making Weights on Individual Features Make Sense\n",
    "Sometimes, the scaling of a feature in the data set may not be a good scale for the models to learn on. In some applications, we take logs on a feature to make it make more sense. (TODO(scan33scan33): add example applications.)\n",
    "If I want to apply log to the feature zero, here is how I will do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: 0.96\n"
     ]
    }
   ],
   "source": [
    "def ApplyFuncToDim(features, func, dim):\n",
    "    features_ret = []\n",
    "    for feature in features:\n",
    "        feature_ret = feature.copy()\n",
    "        feature_ret[dim] = func(feature_ret[dim])\n",
    "        features_ret.append(feature_ret)\n",
    "    return features_ret\n",
    "import math\n",
    "temp_features = ApplyFuncToDim(features, lambda x: math.log(x + 1), 0)\n",
    "print \"Cross-validation accuracies:\", DoCrossValidation(classifier, temp_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "#### Just Quantize!\n",
    "#### Quantized Categorical Features\n",
    "#### Quantization with Prior Knowledge\n",
    "### Dealing with Missing Values\n",
    "#### Imputation\n",
    "#### Missing Value Indicator\n",
    "### Learning higher-order correlations\n",
    "#### Polynomial of Degree-n Expansion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
